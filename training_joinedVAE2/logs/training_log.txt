Device: cuda
Model size: 15238723 

Model weight LatentVAE1_256_128.pth loaded. 

n_train: 144911, n_val: 7945
Epoch 1/30:
   Step 96/567: loss=1.2717, recon=0.4744, KL=0.6135, latent=0.1838
   Step 192/567: loss=1.2250, recon=0.4431, KL=0.6292, latent=0.1527
   Step 288/567: loss=1.2164, recon=0.4397, KL=0.6394, latent=0.1372
   Step 384/567: loss=1.2286, recon=0.4492, KL=0.6458, latent=0.1337
   Step 480/567: loss=1.1879, recon=0.4158, KL=0.6490, latent=0.1231
Train Loss: 1.2320 (Recon: 0.4447, KL: 0.6291, Latent: 0.1581) | Val Loss: 1.3815 (Recon: 0.5869, KL: 0.6589, Latent: 0.1356)

   Checkpoint saved at epoch 1 with loss_val=1.3815
Epoch 2/30:
   Step 96/567: loss=1.1976, recon=0.4222, KL=0.6546, latent=0.1209
   Step 192/567: loss=1.1995, recon=0.4249, KL=0.6540, latent=0.1206
   Step 288/567: loss=1.1965, recon=0.4208, KL=0.6567, latent=0.1190
   Step 384/567: loss=1.1930, recon=0.4172, KL=0.6572, latent=0.1187
   Step 480/567: loss=1.2027, recon=0.4222, KL=0.6627, latent=0.1178
Train Loss: 1.1947 (Recon: 0.4198, KL: 0.6569, Latent: 0.1181) | Val Loss: 1.3611 (Recon: 0.5667, KL: 0.6689, Latent: 0.1255)

   Checkpoint saved at epoch 2 with loss_val=1.3611
Epoch 3/30:
   Step 96/567: loss=1.2074, recon=0.4263, KL=0.6646, latent=0.1165
   Step 192/567: loss=1.1915, recon=0.4192, KL=0.6623, latent=0.1100
   Step 288/567: loss=1.2088, recon=0.4278, KL=0.6637, latent=0.1173
   Step 384/567: loss=1.1629, recon=0.3922, KL=0.6610, latent=0.1097
   Step 480/567: loss=1.1963, recon=0.4173, KL=0.6651, latent=0.1139
Train Loss: 1.1833 (Recon: 0.4093, KL: 0.6619, Latent: 0.1121) | Val Loss: 1.3506 (Recon: 0.5581, KL: 0.6716, Latent: 0.1208)

   Checkpoint saved at epoch 3 with loss_val=1.3506
Epoch 4/30:
   Step 96/567: loss=1.1851, recon=0.4108, KL=0.6640, latent=0.1103
   Step 192/567: loss=1.1579, recon=0.3869, KL=0.6610, latent=0.1100
   Step 288/567: loss=1.1636, recon=0.3892, KL=0.6646, latent=0.1098
   Step 384/567: loss=1.1621, recon=0.3952, KL=0.6617, latent=0.1052
   Step 480/567: loss=1.1928, recon=0.4176, KL=0.6644, latent=0.1109
Train Loss: 1.1750 (Recon: 0.4017, KL: 0.6640, Latent: 0.1093) | Val Loss: 1.3429 (Recon: 0.5510, KL: 0.6729, Latent: 0.1189)

   Checkpoint saved at epoch 4 with loss_val=1.3429
Epoch 5/30:
   Step 96/567: loss=1.1937, recon=0.4163, KL=0.6668, latent=0.1106
   Step 192/567: loss=1.1939, recon=0.4133, KL=0.6680, latent=0.1126
   Step 288/567: loss=1.1789, recon=0.4021, KL=0.6679, latent=0.1088
   Step 384/567: loss=1.1730, recon=0.3974, KL=0.6667, latent=0.1090
   Step 480/567: loss=1.1743, recon=0.3998, KL=0.6673, latent=0.1072
Train Loss: 1.1687 (Recon: 0.3959, KL: 0.6652, Latent: 0.1076) | Val Loss: 1.3322 (Recon: 0.5399, KL: 0.6753, Latent: 0.1170)

   Checkpoint saved at epoch 5 with loss_val=1.3322
Epoch 6/30:
   Step 96/567: loss=1.1798, recon=0.4117, KL=0.6647, latent=0.1033
   Step 192/567: loss=1.1653, recon=0.3972, KL=0.6643, latent=0.1038
   Step 288/567: loss=1.1709, recon=0.4022, KL=0.6649, latent=0.1038
   Step 384/567: loss=1.1540, recon=0.3851, KL=0.6638, latent=0.1051
   Step 480/567: loss=1.1535, recon=0.3878, KL=0.6655, latent=0.1001
Train Loss: 1.1629 (Recon: 0.3908, KL: 0.6659, Latent: 0.1063) | Val Loss: 1.3266 (Recon: 0.5360, KL: 0.6754, Latent: 0.1152)

   Checkpoint saved at epoch 6 with loss_val=1.3266
Epoch 7/30:
   Step 96/567: loss=1.1713, recon=0.3908, KL=0.6693, latent=0.1112
   Step 192/567: loss=1.1658, recon=0.3957, KL=0.6678, latent=0.1024
   Step 288/567: loss=1.1835, recon=0.4034, KL=0.6714, latent=0.1086
   Step 384/567: loss=1.1649, recon=0.3947, KL=0.6672, latent=0.1031
   Step 480/567: loss=1.1684, recon=0.3956, KL=0.6649, latent=0.1079
Train Loss: 1.1580 (Recon: 0.3864, KL: 0.6662, Latent: 0.1054) | Val Loss: 1.3214 (Recon: 0.5306, KL: 0.6756, Latent: 0.1151)

   Checkpoint saved at epoch 7 with loss_val=1.3214
Epoch 8/30:
   Step 96/567: loss=1.1391, recon=0.3702, KL=0.6653, latent=0.1037
   Step 192/567: loss=1.1650, recon=0.3915, KL=0.6677, latent=0.1058
   Step 288/567: loss=1.1593, recon=0.3830, KL=0.6689, latent=0.1074
   Step 384/567: loss=1.1622, recon=0.3893, KL=0.6678, latent=0.1051
   Step 480/567: loss=1.1494, recon=0.3774, KL=0.6677, latent=0.1043
Train Loss: 1.1535 (Recon: 0.3823, KL: 0.6665, Latent: 0.1046) | Val Loss: 1.3157 (Recon: 0.5253, KL: 0.6773, Latent: 0.1131)

   Checkpoint saved at epoch 8 with loss_val=1.3157
Epoch 9/30:
   Step 96/567: loss=1.1459, recon=0.3755, KL=0.6676, latent=0.1029
   Step 192/567: loss=1.1579, recon=0.3881, KL=0.6662, latent=0.1036
   Step 288/567: loss=1.1286, recon=0.3614, KL=0.6635, latent=0.1037
   Step 384/567: loss=1.1634, recon=0.3887, KL=0.6674, latent=0.1073
   Step 480/567: loss=1.1577, recon=0.3801, KL=0.6705, latent=0.1072
Train Loss: 1.1497 (Recon: 0.3787, KL: 0.6670, Latent: 0.1040) | Val Loss: 1.3158 (Recon: 0.5252, KL: 0.6775, Latent: 0.1132)

Epoch 10/30:
   Step 96/567: loss=1.1358, recon=0.3636, KL=0.6661, latent=0.1061
   Step 192/567: loss=1.1606, recon=0.3852, KL=0.6679, latent=0.1075
   Step 288/567: loss=1.1311, recon=0.3647, KL=0.6662, latent=0.1001
   Step 384/567: loss=1.1461, recon=0.3727, KL=0.6675, latent=0.1059
   Step 480/567: loss=1.1345, recon=0.3638, KL=0.6678, latent=0.1029
Train Loss: 1.1458 (Recon: 0.3752, KL: 0.6671, Latent: 0.1035) | Val Loss: 1.3069 (Recon: 0.5173, KL: 0.6770, Latent: 0.1126)

   Checkpoint saved at epoch 10 with loss_val=1.3069
Epoch 11/30:
   Step 96/567: loss=1.1233, recon=0.3593, KL=0.6642, latent=0.0998
   Step 192/567: loss=1.1213, recon=0.3564, KL=0.6637, latent=0.1013
   Step 288/567: loss=1.1357, recon=0.3670, KL=0.6645, latent=0.1042
   Step 384/567: loss=1.1441, recon=0.3710, KL=0.6691, latent=0.1040
   Step 480/567: loss=1.1258, recon=0.3557, KL=0.6666, latent=0.1035
Train Loss: 1.1424 (Recon: 0.3721, KL: 0.6672, Latent: 0.1031) | Val Loss: 1.3016 (Recon: 0.5136, KL: 0.6763, Latent: 0.1117)

   Checkpoint saved at epoch 11 with loss_val=1.3016
Epoch 12/30:
   Step 96/567: loss=1.1259, recon=0.3646, KL=0.6624, latent=0.0989
   Step 192/567: loss=1.1420, recon=0.3719, KL=0.6664, latent=0.1038
   Step 288/567: loss=1.1356, recon=0.3673, KL=0.6687, latent=0.0996
   Step 384/567: loss=1.1576, recon=0.3851, KL=0.6685, latent=0.1040
   Step 480/567: loss=1.1201, recon=0.3623, KL=0.6633, latent=0.0946
Train Loss: 1.1390 (Recon: 0.3691, KL: 0.6673, Latent: 0.1026) | Val Loss: 1.3100 (Recon: 0.5206, KL: 0.6778, Latent: 0.1116)

Epoch 13/30:
   Step 96/567: loss=1.1288, recon=0.3627, KL=0.6648, latent=0.1013
   Step 192/567: loss=1.1331, recon=0.3640, KL=0.6649, latent=0.1042
   Step 288/567: loss=1.1342, recon=0.3637, KL=0.6697, latent=0.1008
   Step 384/567: loss=1.1364, recon=0.3741, KL=0.6636, latent=0.0987
   Step 480/567: loss=1.1162, recon=0.3498, KL=0.6663, latent=0.1001
Train Loss: 1.1363 (Recon: 0.3666, KL: 0.6673, Latent: 0.1023) | Val Loss: 1.2971 (Recon: 0.5071, KL: 0.6787, Latent: 0.1113)

   Checkpoint saved at epoch 13 with loss_val=1.2971
Epoch 14/30:
   Step 96/567: loss=1.1258, recon=0.3611, KL=0.6657, latent=0.0990
   Step 192/567: loss=1.1369, recon=0.3640, KL=0.6692, latent=0.1037
   Step 288/567: loss=1.1293, recon=0.3584, KL=0.6675, latent=0.1034
   Step 384/567: loss=1.1310, recon=0.3607, KL=0.6701, latent=0.1002
   Step 480/567: loss=1.1427, recon=0.3694, KL=0.6700, latent=0.1033
Train Loss: 1.1331 (Recon: 0.3638, KL: 0.6674, Latent: 0.1019) | Val Loss: 1.2930 (Recon: 0.5034, KL: 0.6785, Latent: 0.1110)

   Checkpoint saved at epoch 14 with loss_val=1.2930
Epoch 15/30:
   Step 96/567: loss=1.1290, recon=0.3594, KL=0.6669, latent=0.1027
   Step 192/567: loss=1.1391, recon=0.3642, KL=0.6711, latent=0.1037
   Step 288/567: loss=1.1424, recon=0.3680, KL=0.6708, latent=0.1036
   Step 384/567: loss=1.1328, recon=0.3619, KL=0.6681, latent=0.1028
   Step 480/567: loss=1.1174, recon=0.3478, KL=0.6676, latent=0.1021
Train Loss: 1.1301 (Recon: 0.3612, KL: 0.6672, Latent: 0.1017) | Val Loss: 1.2897 (Recon: 0.5032, KL: 0.6771, Latent: 0.1094)

   Checkpoint saved at epoch 15 with loss_val=1.2897
Epoch 16/30:
   Step 96/567: loss=1.1208, recon=0.3545, KL=0.6645, latent=0.1018
   Step 192/567: loss=1.1231, recon=0.3555, KL=0.6684, latent=0.0992
   Step 288/567: loss=1.1362, recon=0.3680, KL=0.6677, latent=0.1006
   Step 384/567: loss=1.1131, recon=0.3448, KL=0.6668, latent=0.1016
   Step 480/567: loss=1.1205, recon=0.3579, KL=0.6655, latent=0.0972
Train Loss: 1.1278 (Recon: 0.3591, KL: 0.6673, Latent: 0.1014) | Val Loss: 1.2852 (Recon: 0.4967, KL: 0.6786, Latent: 0.1099)

   Checkpoint saved at epoch 16 with loss_val=1.2852
Epoch 17/30:
   Step 96/567: loss=1.1192, recon=0.3499, KL=0.6658, latent=0.1035
   Step 192/567: loss=1.1126, recon=0.3472, KL=0.6653, latent=0.1001
   Step 288/567: loss=1.1241, recon=0.3567, KL=0.6668, latent=0.1006
   Step 384/567: loss=1.1335, recon=0.3658, KL=0.6676, latent=0.1001
   Step 480/567: loss=1.1342, recon=0.3638, KL=0.6695, latent=0.1010
Train Loss: 1.1253 (Recon: 0.3570, KL: 0.6670, Latent: 0.1012) | Val Loss: 1.2835 (Recon: 0.4968, KL: 0.6773, Latent: 0.1094)

   Checkpoint saved at epoch 17 with loss_val=1.2835
Epoch 18/30:
   Step 96/567: loss=1.1301, recon=0.3604, KL=0.6683, latent=0.1013
   Step 192/567: loss=1.1230, recon=0.3579, KL=0.6650, latent=0.1001
   Step 288/567: loss=1.1246, recon=0.3546, KL=0.6671, latent=0.1029
   Step 384/567: loss=1.1149, recon=0.3439, KL=0.6686, latent=0.1024
   Step 480/567: loss=1.1134, recon=0.3516, KL=0.6652, latent=0.0966
Train Loss: 1.1229 (Recon: 0.3550, KL: 0.6668, Latent: 0.1011) | Val Loss: 1.2832 (Recon: 0.4964, KL: 0.6774, Latent: 0.1094)

   Checkpoint saved at epoch 18 with loss_val=1.2832
Epoch 19/30:
   Step 96/567: loss=1.1191, recon=0.3506, KL=0.6658, latent=0.1026
   Step 192/567: loss=1.1261, recon=0.3531, KL=0.6684, latent=0.1047
   Step 288/567: loss=1.1366, recon=0.3604, KL=0.6697, latent=0.1065
   Step 384/567: loss=1.1101, recon=0.3468, KL=0.6645, latent=0.0987
   Step 480/567: loss=1.1223, recon=0.3492, KL=0.6700, latent=0.1031
Train Loss: 1.1202 (Recon: 0.3528, KL: 0.6666, Latent: 0.1008) | Val Loss: 1.2758 (Recon: 0.4897, KL: 0.6768, Latent: 0.1094)

   Checkpoint saved at epoch 19 with loss_val=1.2758
Epoch 20/30:
   Step 96/567: loss=1.1326, recon=0.3660, KL=0.6669, latent=0.0997
   Step 192/567: loss=1.1230, recon=0.3510, KL=0.6709, latent=0.1010
   Step 288/567: loss=1.1078, recon=0.3431, KL=0.6666, latent=0.0981
   Step 384/567: loss=1.1030, recon=0.3369, KL=0.6669, latent=0.0991
   Step 480/567: loss=1.1103, recon=0.3457, KL=0.6640, latent=0.1006
Train Loss: 1.1178 (Recon: 0.3507, KL: 0.6664, Latent: 0.1007) | Val Loss: 1.2752 (Recon: 0.4895, KL: 0.6764, Latent: 0.1093)

   Checkpoint saved at epoch 20 with loss_val=1.2752
Epoch 21/30:
   Step 96/567: loss=1.1078, recon=0.3357, KL=0.6676, latent=0.1045
   Step 192/567: loss=1.1238, recon=0.3552, KL=0.6682, latent=0.1004
   Step 288/567: loss=1.1286, recon=0.3569, KL=0.6672, latent=0.1044
   Step 384/567: loss=1.1242, recon=0.3519, KL=0.6692, latent=0.1031
   Step 480/567: loss=1.1064, recon=0.3377, KL=0.6676, latent=0.1011
Train Loss: 1.1155 (Recon: 0.3488, KL: 0.6660, Latent: 0.1006) | Val Loss: 1.2730 (Recon: 0.4860, KL: 0.6779, Latent: 0.1091)

   Checkpoint saved at epoch 21 with loss_val=1.2730
Epoch 22/30:
   Step 96/567: loss=1.1081, recon=0.3421, KL=0.6652, latent=0.1009
   Step 192/567: loss=1.1166, recon=0.3449, KL=0.6682, latent=0.1034
   Step 288/567: loss=1.1223, recon=0.3539, KL=0.6670, latent=0.1015
   Step 384/567: loss=1.1202, recon=0.3512, KL=0.6682, latent=0.1009
   Step 480/567: loss=1.1194, recon=0.3518, KL=0.6666, latent=0.1011
Train Loss: 1.1146 (Recon: 0.3479, KL: 0.6661, Latent: 0.1006) | Val Loss: 1.2733 (Recon: 0.4868, KL: 0.6767, Latent: 0.1099)

Epoch 23/30:
   Step 96/567: loss=1.1120, recon=0.3461, KL=0.6640, latent=0.1019
   Step 192/567: loss=1.1161, recon=0.3515, KL=0.6658, latent=0.0987
   Step 288/567: loss=1.1052, recon=0.3368, KL=0.6676, latent=0.1008
   Step 384/567: loss=1.0979, recon=0.3332, KL=0.6651, latent=0.0997
   Step 480/567: loss=1.1201, recon=0.3533, KL=0.6673, latent=0.0995
Train Loss: 1.1121 (Recon: 0.3458, KL: 0.6657, Latent: 0.1005) | Val Loss: 1.2659 (Recon: 0.4804, KL: 0.6765, Latent: 0.1089)

   Checkpoint saved at epoch 23 with loss_val=1.2659
Epoch 24/30:
   Step 96/567: loss=1.1031, recon=0.3404, KL=0.6615, latent=0.1012
   Step 192/567: loss=1.1190, recon=0.3524, KL=0.6666, latent=0.1001
   Step 288/567: loss=1.1153, recon=0.3525, KL=0.6635, latent=0.0993
   Step 384/567: loss=1.1271, recon=0.3573, KL=0.6666, latent=0.1032
   Step 480/567: loss=1.1015, recon=0.3402, KL=0.6637, latent=0.0976
Train Loss: 1.1101 (Recon: 0.3442, KL: 0.6654, Latent: 0.1005) | Val Loss: 1.2662 (Recon: 0.4818, KL: 0.6759, Latent: 0.1086)

Epoch 25/30:
   Step 96/567: loss=1.0985, recon=0.3298, KL=0.6650, latent=0.1036
   Step 192/567: loss=1.1025, recon=0.3417, KL=0.6646, latent=0.0961
   Step 288/567: loss=1.1084, recon=0.3431, KL=0.6648, latent=0.1005
   Step 384/567: loss=1.1186, recon=0.3499, KL=0.6660, latent=0.1027
   Step 480/567: loss=1.1018, recon=0.3328, KL=0.6642, latent=0.1048
Train Loss: 1.1082 (Recon: 0.3427, KL: 0.6652, Latent: 0.1004) | Val Loss: 1.2668 (Recon: 0.4825, KL: 0.6755, Latent: 0.1088)

Epoch 26/30:
   Step 96/567: loss=1.1063, recon=0.3446, KL=0.6633, latent=0.0983
   Step 192/567: loss=1.1033, recon=0.3425, KL=0.6622, latent=0.0985
   Step 288/567: loss=1.1096, recon=0.3480, KL=0.6630, latent=0.0985
   Step 384/567: loss=1.1107, recon=0.3413, KL=0.6669, latent=0.1025
   Step 480/567: loss=1.1017, recon=0.3353, KL=0.6663, latent=0.1001
Train Loss: 1.1061 (Recon: 0.3408, KL: 0.6651, Latent: 0.1002) | Val Loss: 1.2590 (Recon: 0.4748, KL: 0.6756, Latent: 0.1086)

   Checkpoint saved at epoch 26 with loss_val=1.2590
Epoch 27/30:
   Step 96/567: loss=1.1043, recon=0.3436, KL=0.6644, latent=0.0963
   Step 192/567: loss=1.0911, recon=0.3266, KL=0.6635, latent=0.1009
   Step 288/567: loss=1.0916, recon=0.3283, KL=0.6638, latent=0.0996
   Step 384/567: loss=1.1108, recon=0.3456, KL=0.6658, latent=0.0994
   Step 480/567: loss=1.0888, recon=0.3256, KL=0.6640, latent=0.0991
Train Loss: 1.1048 (Recon: 0.3398, KL: 0.6648, Latent: 0.1003) | Val Loss: 1.2603 (Recon: 0.4759, KL: 0.6755, Latent: 0.1089)

Epoch 28/30:
   Step 96/567: loss=1.1030, recon=0.3375, KL=0.6629, latent=0.1027
   Step 192/567: loss=1.1216, recon=0.3543, KL=0.6663, latent=0.1009
   Step 288/567: loss=1.1029, recon=0.3359, KL=0.6659, latent=0.1012
   Step 384/567: loss=1.0854, recon=0.3251, KL=0.6605, latent=0.0998
   Step 480/567: loss=1.0815, recon=0.3262, KL=0.6609, latent=0.0945
Train Loss: 1.1028 (Recon: 0.3380, KL: 0.6647, Latent: 0.1001) | Val Loss: 1.2578 (Recon: 0.4732, KL: 0.6764, Latent: 0.1083)

   Checkpoint saved at epoch 28 with loss_val=1.2578
Epoch 29/30:
   Step 96/567: loss=1.1129, recon=0.3405, KL=0.6701, latent=0.1022
   Step 192/567: loss=1.0929, recon=0.3282, KL=0.6631, latent=0.1016
   Step 288/567: loss=1.0763, recon=0.3194, KL=0.6602, latent=0.0966
   Step 384/567: loss=1.1030, recon=0.3386, KL=0.6654, latent=0.0990
   Step 480/567: loss=1.1027, recon=0.3363, KL=0.6646, latent=0.1018
Train Loss: 1.1007 (Recon: 0.3363, KL: 0.6644, Latent: 0.0999) | Val Loss: 1.2560 (Recon: 0.4698, KL: 0.6772, Latent: 0.1090)

   Checkpoint saved at epoch 29 with loss_val=1.2560
Epoch 30/30:
   Step 96/567: loss=1.1000, recon=0.3360, KL=0.6638, latent=0.1002
   Step 192/567: loss=1.1126, recon=0.3477, KL=0.6659, latent=0.0989
   Step 288/567: loss=1.0827, recon=0.3222, KL=0.6601, latent=0.1004
   Step 384/567: loss=1.1087, recon=0.3464, KL=0.6622, latent=0.1000
   Step 480/567: loss=1.0871, recon=0.3259, KL=0.6634, latent=0.0978
Train Loss: 1.0991 (Recon: 0.3350, KL: 0.6642, Latent: 0.1000) | Val Loss: 1.2537 (Recon: 0.4703, KL: 0.6754, Latent: 0.1080)

   Checkpoint saved at epoch 30 with loss_val=1.2537
