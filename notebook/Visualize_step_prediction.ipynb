{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b1e7b7-2a7f-4006-93d7-6f41aec2fd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 22-082_10X_DAPI_LAM_DYS_COL4_1-Scene-3-TAG03 and 22-082_10X_NADH_9-Scene-3-TAG03\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import sys\n",
    "sys.path.append(\"/media/DATABRUT/DB_DDC/serverGPU/Cache_GPU_Ai/fiber_matcher/FFMatcher/training_match/\")\n",
    "\n",
    "from configs import *\n",
    "from FM_match import *\n",
    "from helper import *\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "set_seed(random_seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# change the directory to save the outputs\n",
    "from run_match_input import *\n",
    "\n",
    "# Create the directories\n",
    "images_dir = f\"{dir_save_output}/images\" #dir of images used in training\n",
    "dir_tempo = f\"{dir_save_output}/tempo/\" #dir to dump the temporary files\n",
    "dir_embedding = f\"{dir_save_output}/VAE_embed\"\n",
    "dir_save_npz = f\"{dir_save_output}/npz_256\"\n",
    "dir_save_prediction_output = f\"{dir_save_output}/prediction_output\"\n",
    "dir_save_cellpose_masks = f\"{dir_save_output}/out_CP_masks\"\n",
    "\n",
    "for d in [dir_save_output, images_dir, dir_tempo, dir_embedding, dir_save_npz, dir_save_prediction_output, dir_save_cellpose_masks]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# IHF or not\n",
    "IHF1 = param_img1[0]==\"fluorescence\"\n",
    "IHF2 = param_img2[0]==\"fluorescence\"\n",
    "\n",
    "img1 = \"22-082_10X_DAPI_LAM_DYS_COL4_1-Scene-3-TAG03\"\n",
    "img2 = \"22-082_10X_NADH_9-Scene-3-TAG03\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b652e-60cc-4d7e-a336-4c2a2f13e52f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Save step prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b9df0a-841c-4ada-bed3-0cbbf949de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_prediction_single_pair = f\"{dir_save_prediction_output}/{img1}___vs___{img2}\"\n",
    "\n",
    "with open(f\"{dir_prediction_single_pair}/step_prediction.pkl\", \"rb\") as f:\n",
    "    step_prediction = pickle.load(f)\n",
    "\n",
    "masks1, _, _ = segment_image(\n",
    "    img_path=f\"{images_dir}/{img1}.png\",  \n",
    "    CP_model_name=CP_model_name_1,\n",
    "    savedir=dir_save_cellpose_masks\n",
    ")\n",
    "\n",
    "masks2, _, _ = segment_image(\n",
    "    img_path=f\"{images_dir}/{img2}.png\",  \n",
    "    CP_model_name=CP_model_name_2,\n",
    "    savedir=dir_save_cellpose_masks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7402233-9fda-4288-9a88-e7e1e5fa0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [img1, img2]\n",
    "masks = [masks1, masks2]\n",
    "IHFs = [IHF1, IHF2]\n",
    "\n",
    "# Start prediction\n",
    "for img_index in range(2):\n",
    "    labels_step = []\n",
    "    list_roi_plot = [l[img_index] for l in labels_step]\n",
    "    draw_instructions = get_list_params_contours(list_roi_plot, masks[img_index], IHFs[img_index], \"on-going\")\n",
    "    save_image_per_step(\n",
    "        imgs[img_index], images_dir, \n",
    "        draw_instructions,\n",
    "        dir_prediction_single_pair,\n",
    "        suffix=\"Step0\",\n",
    "        dpi=50,\n",
    "    )\n",
    "\n",
    "# 1_initial_guess\n",
    "for img_index in range(2):\n",
    "    labels_step = step_prediction['1_initial_guess']\n",
    "    list_roi_plot = [l[img_index] for l in labels_step]\n",
    "    draw_instructions = get_list_params_contours(list_roi_plot, masks[img_index], IHFs[img_index], \"on-going\")\n",
    "    save_image_per_step(\n",
    "        imgs[img_index], images_dir, \n",
    "        draw_instructions,\n",
    "        dir_prediction_single_pair,\n",
    "        suffix=\"Step1\",\n",
    "        dpi=50,\n",
    "    )\n",
    "\n",
    "# 2_selected_combs_from_initial_guess\n",
    "for img_index in range(2):\n",
    "    labels_step = step_prediction['2_selected_combs_from_initial_guess']\n",
    "    list_roi_plot = [l[img_index] for l in labels_step]\n",
    "    draw_instructions = get_list_params_contours(list_roi_plot, masks[img_index], IHFs[img_index], \"confirmed\")\n",
    "    save_image_per_step(\n",
    "        imgs[img_index], images_dir, \n",
    "        draw_instructions,\n",
    "        dir_prediction_single_pair,\n",
    "        suffix=\"Step2\",\n",
    "        dpi=50,\n",
    "    )\n",
    "\n",
    "# 3_local_prediction\n",
    "for img_index in range(2):\n",
    "    for i,labels_step in enumerate(step_prediction['3_local_prediction']):\n",
    "        list_roi_plot = [l[img_index] for l in labels_step]\n",
    "        draw_instructions = get_list_params_contours(list_roi_plot, masks[img_index], IHFs[img_index], \"confirmed\")\n",
    "        save_image_per_step(\n",
    "            imgs[img_index], images_dir, \n",
    "            draw_instructions,\n",
    "            dir_prediction_single_pair,\n",
    "            suffix=f\"Step3-{i}\",\n",
    "            dpi=50,\n",
    "        )\n",
    "\n",
    "# 4_unannotated_prediction\n",
    "for img_index in range(2):\n",
    "    labels_step = step_prediction['4_unannotated_prediction'][-1]\n",
    "    list_roi_plot = [l[img_index] for l in labels_step]\n",
    "    draw_instructions = get_list_params_contours(list_roi_plot, masks[img_index], IHFs[img_index], \"confirmed\")\n",
    "    save_image_per_step(\n",
    "        imgs[img_index], images_dir, \n",
    "        draw_instructions,\n",
    "        dir_prediction_single_pair,\n",
    "        suffix=f\"Step4\",\n",
    "        dpi=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a35b74-cb78-4992-97f2-5ecd7380da57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Make time-step video for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e265b90a-30ef-4331-b1f8-797999b18347",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_images, right_images = [], []\n",
    "suffixes = [\"Step0\", \"Step1\", \"Step2\"]\n",
    "for i in range(len(step_prediction['3_local_prediction'])):\n",
    "    suffixes.append(f\"Step3-{i}\")\n",
    "suffixes.append(\"Step4\")\n",
    "\n",
    "for suffix in suffixes:\n",
    "    left_images.append(f\"{dir_prediction_single_pair}/{img1}_{suffix}.png\")\n",
    "    right_images.append(f\"{dir_prediction_single_pair}/{img2}_{suffix}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ccd68f-499b-4765-a7de-404a8a7bc302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 15 frames → comparison.mp4 (1565x800@2fps)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def _load_rgb(path):\n",
    "    # PIL avoids cv2’s BGR gotcha\n",
    "    return np.array(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "def _resize_keep_aspect(img, target_h=None, target_w=None):\n",
    "    h, w = img.shape[:2]\n",
    "    if target_h is not None:\n",
    "        scale = target_h / h\n",
    "    elif target_w is not None:\n",
    "        scale = target_w / w\n",
    "    else:\n",
    "        return img\n",
    "    new_w, new_h = int(round(w*scale)), int(round(h*scale))\n",
    "    return np.array(Image.fromarray(img).resize((new_w, new_h), Image.BILINEAR))\n",
    "\n",
    "def _pad_to_same_height(img_left, img_right, pad_color=(0,0,0)):\n",
    "    hL, wL = img_left.shape[:2]\n",
    "    hR, wR = img_right.shape[:2]\n",
    "    H = max(hL, hR)\n",
    "    if hL < H:\n",
    "        pad = np.full((H - hL, wL, 3), pad_color, dtype=np.uint8)\n",
    "        img_left = np.vstack([img_left, pad])\n",
    "    if hR < H:\n",
    "        pad = np.full((H - hR, wR, 3), pad_color, dtype=np.uint8)\n",
    "        img_right = np.vstack([img_right, pad])\n",
    "    return img_left, img_right\n",
    "\n",
    "def _annotate(img, text, color, font_scale=1, thickness=2, margin=6):\n",
    "    overlay = img.copy()\n",
    "    cv2.putText(\n",
    "        overlay, text, (margin, margin + 24),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA\n",
    "    )\n",
    "    return overlay\n",
    "\n",
    "def make_side_by_side_video(\n",
    "    pairs,                     # list of (path_left, path_right)\n",
    "    out_path=\"out.mp4\",\n",
    "    fps=10,\n",
    "    target_height=720,         # final frame height\n",
    "    add_filenames=True,\n",
    "    left_title=\"Laminin\",           # e.g. \"Slide A\"\n",
    "    right_title=\"NADH\",          # e.g. \"Slide B\"\n",
    "    fourcc=\"mp4v\",             # \"mp4v\" (broad) or \"avc1\" (H.264 if available)\n",
    "):\n",
    "    pairs = [(Path(a), Path(b)) for a,b in pairs]\n",
    "    assert len(pairs) > 0, \"No input pairs.\"\n",
    "\n",
    "    frames = []\n",
    "    for a, b in pairs:\n",
    "        L = _load_rgb(a)\n",
    "        R = _load_rgb(b)\n",
    "\n",
    "        # 1) scale both to the same target height\n",
    "        L = _resize_keep_aspect(L, target_h=target_height)\n",
    "        R = _resize_keep_aspect(R, target_h=target_height)\n",
    "\n",
    "        # 2) pad to identical height (if rounding created a 1px diff)\n",
    "        L, R = _pad_to_same_height(L, R)\n",
    "\n",
    "        # 3) optional labels (titles override filenames if given)\n",
    "        if add_filenames:\n",
    "            txtL = left_title if left_title is not None else a.name\n",
    "            txtR = right_title if right_title is not None else b.name\n",
    "            L = _annotate(L, txtL, (255,255,255))\n",
    "            R = _annotate(R, txtR, (0,0,0))\n",
    "\n",
    "        # 4) concatenate horizontally\n",
    "        frame = np.hstack([L, R]).astype(np.uint8)\n",
    "        frames.append(frame)\n",
    "\n",
    "    H, W = frames[0].shape[:2]\n",
    "    writer = cv2.VideoWriter(\n",
    "        str(out_path),\n",
    "        cv2.VideoWriter_fourcc(*fourcc),\n",
    "        fps,\n",
    "        (W, H)\n",
    "    )\n",
    "\n",
    "    for f in frames:\n",
    "        # OpenCV expects BGR\n",
    "        writer.write(cv2.cvtColor(f, cv2.COLOR_RGB2BGR))\n",
    "    writer.release()\n",
    "    print(f\"Saved {len(frames)} frames → {out_path} ({W}x{H}@{fps}fps)\")\n",
    "\n",
    "pairs = list(zip(left_images, right_images))\n",
    "make_side_by_side_video(\n",
    "    pairs, \"comparison.mp4\", fps=2, target_height=800,\n",
    "    left_title=\"Laminin\", right_title=\"NADH\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6f476-911e-4092-9664-cb58ebb31ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
